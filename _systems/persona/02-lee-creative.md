# Persona: Lee (마케팅 배경 / 데이터 중심)

## 👤 기본 정보

**대표명**: 이창의 (Lee the Creative)
**별칭**: "데이터에 빠진 마케터"
**기술 배경**: 마케팅/분석 경험 있음, 코딩 경험 0-1년 (SQL 정도)
**나이대**: 25-35세
**직업**: 마케팅/성장 해킹 프리랜서, 온라인 쇼핑몰 운영자
**월 소입**: 200-400만원
**OS**: Mac (주로) 또는 Windows

---

## 🎯 동기 & 목표

### Why (근본 동기)
```
"고객 데이터가 있는데 정리하는 데 너무 오래 걸려.
 데이터를 빨리 정리하고 분석해야 마케팅 의사결정을 할 수 있는데..."
```

- **핵심 문제**: 데이터 수집 & 정리에 엄청난 시간 소비
- **현재 상황**: 손으로 엑셀에 복사-붙여넣기, 또는 불완전한 자동화 도구 사용
- **시간 낭비**: 주 15-20시간을 데이터 수집/정리에 소비
- **기술 배경**: SQL, 구글 애널리틱스 등은 어느 정도 가능

### What (4주 후 바라는 결과)
```
"3가지 데이터 수집/분석을 자동화해서
 분석에만 집중할 수 있게 하고 싶다"
```

- **목표 1**: SNS 게시물 데이터 자동 수집 (일일)
- **목표 2**: 고객 피드백 자동 감정 분석 (주간)
- **목표 3**: 월간 성과 자동 리포트 생성 (월간)
- **기대 효과**: 주 10-15시간 시간 절약 + 더 나은 의사결정

### Success Criteria
| 지표 | 목표 | 측정 방법 |
|-----|------|---------|
| **자동화 수집량** | 월 1000+ 데이터포인트 자동 수집 | 자동화된 데이터 개수 |
| **정확도** | 오류율 <5% | 수동 검증 vs 자동화 결과 |
| **시간 절약** | 주 12시간 이상 | 데이터 수집 전후 시간 비교 |
| **의사결정 개선** | 인사이트 품질 향상 경험 | 자체 평가 |

---

## 🛠️ 제약사항 & 환경

### 기술적 제약
- ❌ 파이썬은 모르지만, SQL 기초 있음
- ❌ IDE 사용 경험 적음
- ✅ 데이터베이스 개념은 이해함
- ✅ API 개념은 어느 정도 이해 (구글 시트 API 등)
- ✅ 정규표현식(regex) 사용 가능

### 시간 & 환경
- **투자 가능 시간**: 주 6-10시간 (저녁/주말)
- **학습 선호**: "왜 이렇게 하는가"를 이해하고 싶음
- **도움 요청**: 막혀도 먼저 스스로 시도 (자립형 학습자)
- **네트워크**: 높은 인터넷 사용 (데이터 수집)

### 도구 & 접근성
- **장비**: 맥북 Pro 또는 좋은 노트북
- **GPU**: 없어도 괜찮음 (텍스트 기반 작업)
- **API 접근**: 유료 API 여러 개 사용 가능 (월 50-200만원 예산)
- **데이터소스**: 인스타그램, 페이스북, 네이버, 쿠팡 등

---

## 📊 행동 패턴 & 선호도

### 학습 스타일
```
"왜 이렇게 하는지 이해하고 싶어"
"단순 따라하기보다 원리 이해"
"검색해서 직접 배우는 거 좋아함"
```

- **개념 학습**: 중요 (원리를 이해하고 싶음)
- **예제 학습**: 좋아함 (실제 데이터로 테스트)
- **문서 읽기**: 잘함 (영어도 가능)
- **커뮤니티**: 온라인 포럼, GitHub 적극 활용

### 결과 지향성
- **과정 vs 결과**: 70% 결과 / 30% 과정 (과정도 조금 중요)
- **완성도**: 보통 이상 (버그 있으면 수정)
- **확장 가능성**: 중요함 (다음엔 뭘 더할 수 있을까?)
- **재사용성**: 중요함 (템플릿화 원함)

### 커뮤니티 & 독립성
- **혼자 vs 함께**: 기본은 혼자이지만, 어느 정도 커뮤니티 관심 있음
- **성과 공유**: 함 (링크드인, 블로그 등에 공유)
- **경쟁 의식**: 있음 (다른 마케터와 비교)
- **네트워킹**: 중요함 (같은 관심사의 사람들과 연결)

---

## 💔 고통점 (Pain Points)

### 주요 고통점

**1. 데이터 수집의 시간 낭비 (최상위)**
```
"인스타 팔로워들의 댓글을 하나하나 정리하는데,
 이게 자동으로 되면 정말 좋을 텐데..."
```
- 반복성 높음
- 데이터가 커질수록 문제 심화
- 의사결정을 위해 필수

**2. 도구의 한계**
```
"IFTTT나 자파이어는 너무 제한적이야.
 내가 원하는 대로 커스터마이징할 수 없어"
```
- 기존 도구로는 원하는 기능이 없음
- 유료 서비스도 비용 대비 부족

**3. 데이터 품질 문제**
```
"자동 수집한 데이터가 엉망이면
 분석 결과도 엉망이잖아.
 정확도가 중요한데..."
```
- 데이터 검증 시간 필요
- 정확하지 않은 자동화는 오히려 해로움

**4. 실시간성 부족**
```
"아침에 봐야 할 데이터가
 저녁까지 모이면 의미가 없어"
```
- 일일 / 시간당 수집 필요
- 기존 도구는 실시간 처리 어려움

### 시도했던 해결책
- IFTTT 앱 사용 (제한적)
- 자파이어(Zapier) 시도 (비쌈, 월 $20-50)
- Python 튜토리얼 봤지만 완성 못함 (2주 이상 소요)
- 프리랜서 데이터 엔지니어 고용 검토 (비쌈, 月 200만원+)

### 왜 실패했나
- **완전한 커스터마이징 불가**: 기존 도구는 자유도 낮음
- **비용 대비 효과 미흡**: 유료 서비스는 비쌈
- **러닝커브**: 1-2주 배우면 될 줄 알았는데 1-2개월 필요
- **계속 배워야 함**: 한 번 만들고 끝이 아니라 유지보수 필요

---

## 🎓 Lee를 위한 설계 원칙

### 1. 이해할 수 있는 구조 (Comprehensible Architecture)
```
"이게 정확히 뭐 하는 건지, 어떤 단계인지 이해할 수 있게"
```

**설계 방향**:
- ✅ 각 부분이 뭐 하는지 설명 (간단히)
- ✅ 전체 구조 다이어그램 제공
- ✅ "이 라이브러리는 왜 사용하나?"는 설명 (선택)

### 2. 정확성과 검증 (Accuracy & Validation)
```
"자동화되는 데이터가 정확한지 검증하는 과정 필수"
```

**설계 방향**:
- ✅ 샘플 데이터로 먼저 테스트
- ✅ 검증 로직 포함 (예: 수집 데이터 수, 형식 확인)
- ✅ 에러 로그 제공 (뭐가 잘못됐는지 알 수 있게)

### 3. 확장 가능성 (Extensibility)
```
"지금은 인스타만 하지만,
 나중에 페이스북, 틱톡도 추가하고 싶어"
```

**설계 방향**:
- ✅ 모듈화된 구조 (함수별로 분리)
- ✅ 설정 파일로 데이터소스 추가 가능
- ✅ 문서에 "확장하는 방법" 포함

### 4. 정기적 실행 & 자동화 (Scheduled Automation)
```
"매일 밤 12시에 자동으로 수집되고,
 아침에 결과를 이메일로 받고 싶어"
```

**설계 방향**:
- ✅ 크론잡(cron job) 또는 스케줄러 설정
- ✅ 결과 알림 (이메일, 슬랙, 노션 등)
- ✅ 실행 로그 추적

---

## 📈 Usecase 우선순위 (Lee의 경우)

| # | 유형 | Usecase | 난이도 | 효과 | 추천도 |
|---|------|---------|--------|------|--------|
| 1 | 수집 | SNS 데이터 자동 수집 | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 🔴 1순위 |
| 2 | 분석 | 피드백 감정 분석 | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 🔴 1순위 |
| 3 | 리포트 | 월간 성과 리포트 자동 생성 | ⭐⭐⭐ | ⭐⭐⭐⭐ | 🟡 2순위 |
| 4 | 모니터링 | 경쟁사 모니터링 자동화 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | 🟡 2순위 |

---

## ✅ Lee 체크리스트

### Setup
- [ ] `/setup-workspace` 완료 (Intermediate 선택)
- [ ] Python 설치 확인
- [ ] 필요한 API 키 준비 (OpenAI, 등)

### Week 1: 데이터 분석 & 설계
- [ ] 수집하고 싶은 데이터 3가지 정의
- [ ] 각 데이터의 형식/크기 분석
- [ ] `/clarify` 실행 (1개 선택)

### Week 2: 설계 & 아키텍처
- [ ] `/design` 실행 (데이터 흐름 포함)
- [ ] API 검증 (실제 호출 테스트)
- [ ] 검증 로직 설계

### Week 3: 구현 & 테스트
- [ ] `/implement` 단계별 진행
- [ ] 샘플 데이터로 테스트
- [ ] 정확도 검증

### Week 4: 배포 & 모니터링
- [ ] 크론잡 설정 (자동 실행)
- [ ] 결과 알림 설정
- [ ] 다음 데이터소스 추가

---

## 📝 Lee를 위한 Userflow 특징

- **구조 설명**: "이 부분이 API 호출, 이 부분이 데이터 정제"
- **검증 단계**: 각 단계마다 "데이터가 맞는지" 확인
- **API 문서 제공**: 사용하는 API의 기본 문서
- **확장 가이드**: "다른 데이터소스 추가하는 방법"
- **모니터링**: "자동 실행 설정하는 방법"

---

## 🎯 최종 목표

**4주 후 Lee의 모습**:
```
"아, 이제 매일 아침 자동으로 데이터가 모이니까
 분석에만 집중할 수 있겠네.
 정확도도 높고, 내가 원하는 대로 커스터마이징도 되고.
 다음엔 이 로직을 다른 팀원한테도 공유해야지"
```

Lee는 단순히 "자동화를 배운" 게 아니라,
"데이터 기반 의사결정을 가능하게 하는 시스템"을 만들었을 것입니다.
